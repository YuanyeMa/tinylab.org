---
layout: post
author: 'Gao Chengbo'
title: "云计算环境下跨类型存储节点间的磁盘拷贝性能分析与优化"
draft: true
license: "cc-by-nc-nd-4.0"
permalink: /qemu-block-copy/
description: "本文通过火焰图和 top 命令详细分析了 qemu nbd server 的磁盘拷贝性能问题，并提出了两种优化方案。"
category:
  - 虚拟化
  - Qemu
tags:
  - 磁盘拷贝
  - 性能优化
  - nbd server
  - virsh
  - libvirt
---

>
> By 高承博、金琦、刘唐 @ 2020.10.23
>

## 使用场景

云计算环境计算节点上的虚机的磁盘镜像一般不会保存在本地，而是保存在存储节点上。对于一些企业，存储服务并不一定是同一种类，有可能是不同厂家的不同型号，最关键的是，不同种类存储节点没有办法相互同步数据。

在这种情况下，可以使用 qemu 的 nbd 服务，在迁移虚机的同时进行不同型号的存储节点之间的磁盘拷贝。原始的存储数据通过两个计算节点上的 qemu 转发给新的存储节点。如下图所示：

![alt 图1](/wp-content/uploads/2020/10/libvirt/graph1.png)

## 磁盘拷贝相关指令

### qemu 的拷贝磁盘 hmp 命令

目的端 nbd server:

    $ nbd_server_start -a -w IP地址:端口号

源端 nbd client:

    $ drive_mirror -n -f drive-virtio-disk0 nbdm:IP地址:端口号:exportname=drive-virtio-disk0

### libvirt 的磁盘拷贝迁移命令

源端命令：

    $ virsh migrate domainID --live --persistent --copy-storage-all --verbose  qemu+ssh://目的IP/system

## 磁盘拷贝原理分析

对于管理层来说，是调用 `virsh migrate` 命令或者 `virDomianMigrate()` 接口。如“图1”所示，拷贝存储是通过计算节点完成的。

原因是 libvirt 的迁移接口事实上是把磁盘拷贝和迁移虚机两个工作通过一个接口函数完成的。

对于 qemu 来说磁盘拷贝和迁移虚机是两个不同的 hmp 命令，libvirt 是分两步分别调用的 qemu 磁盘拷贝和迁移虚机 hmp 命令，由于虚机迁移属于内存拷贝，不是我们这篇文章分析的范畴，所以下面只写出磁盘拷贝部分流程。

### libvirt 拷贝磁盘的流程图

![alt 图2](/wp-content/uploads/2020/10/libvirt/graph2.png)

### libvirt 迁移虚机流程简介

libvirt 迁移虚机分为五个阶段，分别是：

* begin
* prepare
* perform
* finish
* confirm

其中 begin, preform, confirm 分别发生在 src 端，prepare 和 finish 发生在 dst 端。

### 流程分析

**step1**

libvirtd 的 dst 端在 prepare 阶段调用 `qemuMigrationStartNBDServer()` 发送给 qemu 一个 hmp 命令 `nbd_server_start`，让 src 端的 qemu 开启 nbd server 服务，等待链接接入。

**step2**

libvirtd src 端在 perform 阶段调用 `qemuMigrationDriveMirror` 函数向 qemu 发送 hmp 命令 `drive_mirror` 开始向 dst 端拷贝磁盘。

**step3**

libvirtd src 端在 perform 阶段不停的发送 hmp 命令 `query-block-job` 查询当前的磁盘拷贝有没有完成，当 qemu 创建的拷贝存储的协程 `mirror_run` 完成了整个磁盘的拷贝后将调用 `block_job_event_ready()` 把 `BlockJob->ready` 置为 true。`query-block-job` 查询后返回 `info->ready==TRUE`，认为磁盘拷贝完成可以进行内存的迁移。

注意：这时 `mirror_run` 协程并没有退出，因为在内存迁移的时间段里，虚机有可能还会继续往磁盘里写数据。所以 `mirror_run` 协程并没有结束，继续把脏数据传送给 dst 端。

**step4**

libvirtd src 端在 perform 阶段发送 hmp 命令 migrate 开始内存拷贝。

**step5**

libvirtd 内存拷贝完成后，src 端暂停虚机并发送 hmp 命令 `block-job-cancel` 结束磁盘拷贝。由于虚机这个时候已经暂停，`block-job-cancel` 会一直阻塞，直到把最后的脏数据传送给 dst 端后返回成功。

## nbd server 磁盘拷贝性能问题

计算节点磁盘和内存的迁移拷贝是通过网卡传输的。一般来讲计算节点上用来迁移的网卡至少是 10Gb/s，换算成大 B 也就是 1.2GB/s 左右。我们在迁移过程中通过火焰图和 top 可以看出性能瓶颈：

通过 `sar -n DEV 1 100` 命令检测到迁移时的总带宽几乎到达网卡的总带宽。

![alt 图3](/wp-content/uploads/2020/10/libvirt/graph3.png)

top 显示 CPU 占用率为 100%：

![alt 图4](/wp-content/uploads/2020/10/libvirt/graph4.png)

火焰图分析：

![alt 图5](/wp-content/uploads/2020/10/libvirt/graph5.png)

总结：

这个时候带宽已经达到 10G，CPU 占用率已经到达几乎 100%。那么问题来了，如果网卡的带宽再高怎么办？假如计算节点有一张 25Gb 的网卡可以用来迁移，那这个时候 CPU 岂不是成为了瓶颈了？

## 优化思路

### 单独创建 IOthread 线程

从火焰图可以看出 nbd server 端拷贝磁盘时 CPU 大部分时间是 `nbd_trip()` 用来接收 socket 的数据，然后 `nbd_trip()` 再把接收到的数据通过 `nbd_handle_request()` 写入磁盘。

这里 socket 的接收和写盘都是通过 qemu 的 main 线程完成的，如果 main 线程的 CPU 占用率达到 100% 了，那么带宽就无法再提高。

通过 参考文献[^1] 可以看出 main 线程通过 glib 框架处理挂在 `qemu_aio_context` 的所有异步请求。这里不光是处理 `nbd_trip` 所使用的 socket，还有读写磁盘都是通过 `qemu_aio_context`。所以 main 线程的负载很重，一个 `nbd_trip` 协程就可以让 main 线程吃紧。

因此 main 线程本来就不适合处理这种负载很重的异步请求，所以我们想到了把接收 socket 和写盘分离出主线程的方法。

参考资料[^2] 中介绍了 qemu 本身提供可以将写盘的操作单独放在一个 iothread 中。

**注**：这里我们省略介绍了 socket 是如何挂在 `qemu_aio_context` 下的，其实 nbd server 在进行协商后就会使用磁盘的 `aio_context`，如果这个时候磁盘是 iothread 的 context，那么 socket 将会在 iothread 所在的线程中执行。

参考资料[^2] 中介绍的方法写道，必须由客户侧往 PCI 设备的 `virtio_pci_common_cfg` 的 `device_status` 域写入后才能把磁盘的 `aio_context` 注册到 iothread 里。所以这个方法显然不适合我们的场景，因为我们是在迁移时拷贝磁盘，nbd server 端是 dst 端，迁移未完成 dst 端的虚机是没有执行的。也就是并没有进入 guestOS，就无法写入 `virtio_pci_common_cfg`。

所以这里提到的优化方法是 `nbd_trip` 在接收数据前，要单独为 nbd server 创建一个 `socket_thread` 线程，并且把 socket 绑入 `socket_thread` 线程的 `aio_context`，这样这个 socket 就会单独享有一个 CPU。更改如下：

![alt 图6](/wp-content/uploads/2020/10/libvirt/graph6.png)

### 多 socket 与多线程

由于服务器的 CPU 有的是 2GHZ 左右，接收 socket 能力有限，想要带宽达到 20Gb/s 可能一个线程是不够的，所以第 2 个可能的优化思路是使用多 socket 分摊带宽，通过多线程占用多 CPU 来接收 socket 数据。

大体思路是创建多 socket 并把每一 socket 绑入对应的 `socket_thread` 线程，这里不做详细展开。

## 参考文献

[^1]:[《Qemu IO事件处理框架》](https://www.cnblogs.com/ck1020/p/8782032.html)

[^2]:[《浅析qemu iothread》](https://blog.csdn.net/huang987246510/article/details/93912197#EventNotifier_133)
