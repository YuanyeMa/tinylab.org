# qemu 磁盘拷贝分析及优化
    作者：高承博  金琦  刘唐
## 使用场景
   云计算环境计算节点上的虚机的磁盘镜像一般不会保存在本地，一般都是保存在存储节点上。对于一些企业，存储服务并不一定是同一种类，有可能是不同的厂家不同的型号，最关键的是，这两种存储节点没有办法相互同步数据。

   在这种情况下，可以使用 qemu 的 nbd 服务，在迁移虚机的同时进行不同型号的存储节点之间的磁盘拷贝。原始的存储数据通过两个计算节点上的 qemu 转发给新的存储节点。如下图所示：
   ![alt 图1](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图1.png)

## 拷贝磁盘的相关指令
### qemu 的拷贝磁盘 hmp 命令
目的端 nbd server: 
&emsp; nbd_server_start -a -w IP地址:端口号

源端 nbd client: 
&emsp;  drive_mirror -n -f drive-virtio-disk0 nbdm:IP地址:端口号:exportname=drive-virtio-disk0

### libvirt 的磁盘拷贝迁移命令
源端命令：
&emsp;  virsh migrate domainID --live --persistent --copy-storage-all --verbose  qemu+ssh://目的IP/system

## 磁盘拷贝原理分析
对于管理层来说，是调用 virsh migrate命令或者 virDomianMigrate() 接口。如图1所示，拷贝存储是通过计算节点完成的。原因是 libvirt 的迁移接口事实上是把磁盘拷贝和迁移虚机两个工作通过一个接口函数完成的。对于 qemu 来说磁盘拷贝和迁移虚机是两个不同的 hmp 命令，libvirt 是分两步分别调用的 qemu 磁盘拷贝和迁移虚机 hmp 命令，由于虚机迁移属于内存拷贝，不是我们这篇文章分析的范畴，所以下面只写出磁盘拷贝部分流程。
### libvirt 拷贝磁盘的流程图
![alt 图2](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图2.png)

### libvirt 迁移虚机流程简介
libvirt迁移虚机分为五个阶段，分别是 begain,prepare,perform,finish,confirm。其中 bengin,preform,confirm 分别发生在 src 端，prepare和finish发生在 dst 端。

### 流程分析
* step1: libvirtd的dst端在 prepare 阶段调用 qemuMigrationStartNBDServer() 发送给 qemu 一个hmp命令 nbd_server_start，让 src 端的 qemu 开启 nbd server 服务，等待链接接入。

* step2: libvirtd src 端在 perform 阶段调用 qemuMigrationDriveMirror 函数向 qemu 发送 hmp 命令 drive_mirror 开始向 dst 端拷贝磁盘。

* step3: libvirtd src 端在 perform 阶段不停的发送 hmp 命令 query-block-job 查询当前的磁盘拷贝有没有完成，当 qemu 的创建的拷贝存储的携程 mirror_run 完成了整个磁盘的拷贝后将调用 block_job_event_ready() 把 BlockJob->ready 置为true。query-block-job 查询后返回 info->ready==TRUE，认为磁盘拷贝完成可以进行内存的迁移。
注意：这时 mirror_run 携程并没有退出，因为在内存迁移的时间段里，虚机有可能还会继续往磁盘里写数据。所以 mirror_run 携程并没有结束，继续把脏数据传送给dst端。
* step4: libvirtd src 端在 perform 阶段发送 hmp 命令 migrate 开始内存拷贝。
* step5: libvirtd 内存拷贝完成后，src 端暂停虚机并发送 hmp 命令block-job-cancel 结束磁盘拷贝。由于虚机这个时候已经暂停，block-job-cancel 会一直阻塞，知道把最后的脏数据传送给dst端后返回成功。

## nbd server磁盘拷贝目前的性能问题
计算节点磁盘和内存的迁移拷贝是通过网卡传输的。一般来讲计算节点上用来迁移的网卡至少是10Gb/s，换算成大B也就是1.2GB/s左右。我们在迁移过程中通过火焰图和 top 可以看出性能瓶颈：

通过 sar -n DEV 1 100 命令检测到迁移时的总带宽几乎到达网卡的总带宽。

![alt 图3](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图3.png)


top显示CPU占用率为100%：

![alt 图4](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图4.png)

火焰图分析：

![alt 图5](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图5.png)

总结：这个时候带宽已经达到10G，CPU占用率已经到达几乎100%。那么问题来了，如果网卡的带宽再高怎么办？假如计算节点有一张25Gb的网卡可以用来迁移，那这个时候CPU岂不是成为了瓶颈了？

## 优化思路
### 单独创建IOthread线程
 从火焰图可以看出nbd server端拷贝磁盘时CPU大部分时间是nbd_trip()用来接收socket的数据，然后nbd_trip()再把接收到的数据通过nbd_handle_request()写入磁盘。这里socket的接收和写盘都是通过qemu的main线程完成的,如果main线程的CPU占用率达到100%了，那么带宽就无法再提高。通过参考文献1可以看出main线程通过glib框架处理挂在qemu_aio_context的所有异步请求。这里不光是处理nbd_trip所使用的socket，还有读写磁盘都是通过qemu_aio_context。所以main线程的负载很重，一个nbd_trip协程就可以让main线程吃紧。因此main线程本来就不适合处理这种负载很重的异步请求，所以我们想到了把接收socket和写盘分离出主线程的方法。参考资料2中介绍了qemu本身提供可以将写盘的操作单独放在一个iothread中。注：这里我们省略介绍了socket是如何挂在qemu_aio_context下的，其实nbd server在进行协商后就会使用磁盘的aio_context，如果这个时候磁盘是iothread的context，那么socket将会在iothread所在的线程中执行。

 参考资料2中介绍的方法写道，必须客户侧往PCI设备的virtio_pci_common_cfg的device_status域写入后才能把磁盘的aio_context注册到iothread里。所以这个方法显然不适合我们的场景，因为我们是在迁移时拷贝磁盘，nbd server端是dst端，迁移未完成dst端的虚机是没有执行的。也就是并没有进入guestOS，就无法写入virtio_pci_common_cfg。

 所以这里提到的优化方法是nbd_trip在接收数据前，要单独为nbd server创建一个socket_thread线程，并且把socket绑入socket_thread线程的aio_context，这样这个socket就会单独享有一个CPU。更改如下：

 ![alt 图6](E:/学习资料/自己写的文档/投稿/qemu磁盘拷贝原理分析及优化/图6.png)

### 创建多socket，并把每一socket绑入对应的socket_thread线程
由于服务器的CPU有的是2点多GHZ，接收socket能力有限，想要带宽达到20Gb/s可能一个线程是不够的，第二个提供的优化思路是使用多socket分摊带宽，通过多线程占用多CPU来接收socket数据。

## 参考文献
* 参考资料1《Qemu IO事件处理框架》  
&emsp;  https://www.cnblogs.com/ck1020/p/8782032.html

* 参考资料2 《浅析qemu iothread》 
&emsp;  https://blog.csdn.net/huang987246510/article/details/93912197#EventNotifier_133