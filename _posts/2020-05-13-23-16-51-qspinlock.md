---
layout: post
author: 'Li Hanlu'
title: "一文读懂 qspinlock"
draft: true
license: "cc-by-nc-nd-4.0"
permalink: /qspinlock/
description: "本文基于新版本的 Linux v5.0 内核分析了 qspinlock 工作原理。"
category:
  - ARM
  - 内核同步
tags:
  - 锁机制
  - qspinlock
---

## 前言

去年基于 4.4.188 分析过 qspinlock，本文将基于更新版本的 v5.0 代码重新对 qspinlock 梳理。

图片过多，流量预警！

## 数据结构

qspinlock 用不同的数据结构区分表示全局锁结构和用于本地自旋的数据结构，全局锁结构用 `struct qspinlock` 表示，本地自旋用 `mcs_spinlock` 表示。

### struct mcs_spinlock

一个 CPU 最多同时等待 4 个自旋锁，分别在进程上下文、中断上下文、软中断和 NMI，因此每个 cpu 有一个 `mcs_nodes[MAX_NODES]` 数组，在不考虑虚拟化的情况下 `MAX_NODES` 大小为 4。

    static DEFINE_PER_CPU_ALIGNED(struct mcs_spinlock, mcs_nodes[MAX_NODES]);

next 指向等锁队列的下一个节点，`locked=1` 表示当前节点是等锁队列的队首，`count` 用来表示该 node 在 `mcs_nodes` 数组的 index。

    kernel/locking/mcs_spinlock.h:

    struct mcs_spinlock {
    	struct mcs_spinlock *next;
    	int locked; /* 1 if lock acquired */
    	int count;  /* nesting count, see qspinlock.c */
    };

### struct qspinlock

<img src="https://lihanlu.bj.bcebos.com/struct-qspinlock-mips.png@!rdb" class="full-image" />

`struct qspinlock` 构成如上图，含义如下：

- `locked`： 用来指示锁是否已经被占有，0 表示 unlocked，1 表示 locked 状态；
- `pending`： 对MSC的进一步优化，第 1 个等待自旋锁的 CPU 设置 pending 位后直接在全局锁的 locked 上自旋；
- `tail index`：用于指示队列的尾部节点 `mcs_nodes` 数组的索引；
- `tail cpu (+1)`：尾部节点的处理器编号+1

include/asm-generic/qspinlock_types.h:

    typedef struct qspinlock {
    	union {
    		atomic_t val;

    		/*
    		 * By using the whole 2nd least significant byte for the
    		 * pending bit, we can allow better optimization of the lock
    		 * acquisition for the pending bit holder.
    		 */
    #ifdef __LITTLE_ENDIAN
    		struct {
    			u8	locked;
    			u8	pending;
    		};
    		struct {
    			u16	locked_pending;
    			u16	tail;
    		};
    #else
    		struct {
    			u16	tail;
    			u16	locked_pending;
    		};
    		struct {
    			u8	reserved[2];
    			u8	pending;
    			u8	locked;
    		};
    #endif
    	};
    } arch_spinlock_t;

与传统的 MCS 相比，qspinlock 通过存储队尾 mcs 节点的 cpu 号而不直接存储节点的地址将 spinlock 的结构大小压缩到 32bit，争锁过程大致如下图：
　
<img src="https://lihanlu.bj.bcebos.com/qspinlock1.png@!rdb" class="full-image" />

- 如果 qspinlock 整体 val 为0，说明锁空闲，则 cpu 设置 qspinlock 的 locked 位为1后直接持锁
- 第1个等锁的 cpu 设置 pending 位后在 qspinlock 的 locked 位自旋
- 第2个等锁的 cpu 将 qspinlock 的 `tail index` 和 `tail cpu` 信息指向本地 `mcs_spinlock` ，然后在 qspinlock 的 `locked_pending` 上自旋
- 第n个等锁 cpu 将 qspinlock 的 `tail index` 和 `tail cpu` 信息指向本地 `mcs_spinlock`，并将之前队尾节点的 next 指向自己，然后在本地 `mcs_spinlock` 的 locked 上自旋

## 代码分析

### queued_spin_lock

首先分析上锁过程 `queued_spin_lock`，内存中的锁可能的值如下图：

<img src="https://lihanlu.bj.bcebos.com/qs_3.png@!rdb" class="full-image" />

首先 `atomic_try_cmpxchg_acquire` 比较 `lock->val` 和 0，如相等则 `lock->val` 被赋值为 `_Q_LOCKED_VAL`，并返回0，否则返回内存中 `&lock->val` 的值(上述过程是原子的)。如果返回 0 说明锁可用则直接return，否则调用 `queued_spin_lock_slowpath` 慢速路径。

include/asm-generic/qspinlock.h:

    /**
     * queued_spin_lock - acquire a queued spinlock
     * @lock: Pointer to queued spinlock structure
     */
    static __always_inline void queued_spin_lock(struct qspinlock *lock)
    {
    	u32 val = 0;
    	//当前无cpu持锁，快速路经加锁
    	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
    		return;
    	//否则进入慢速路径等锁
    	queued_spin_lock_slowpath(lock, val);
    }

<img src="https://lihanlu.bj.bcebos.com/qs_4.png@!rdb" class="full-image" />


### queued_spin_lock_slowpath

slowpath 代码分为两部分，标号 queue 之前的 “pending bit optimistic spinning” 和标号后的 “MCS queuing”。

#### pending bit optimistic spinning

进入慢速路径，当前 val 一定不为 `[0,0,0]`，如果 val 为 `[0,1,0]` 说明之前持锁的 cpu 刚释放，pending 的 cpu 即将持锁，状态从 `[0,1,0]` 向 `[0,0,1]` 迁移，那么进行一定次数的自旋等待状态迁移。

这部分的代码改动 commit 为 `59fb586b4a07b4e1a0ee577140ab4842ba451acd`，感兴趣的可以看下log。

kernel/locking/qspinlock.c:

    void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
    {
    	struct mcs_spinlock *prev, *next, *node;
    	u32 old, tail;
    	int idx;

    	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

    	if (pv_enabled())
    		goto pv_queue;

    	if (virt_spin_lock(lock))
    		return;

    	/*
    	 * Wait for in-progress pending->locked hand-overs with a bounded
    	 * number of spins so that we guarantee forward progress.
    	 *
    	 * 0,1,0 -> 0,0,1
    	 */
    	if (val == _Q_PENDING_VAL) {
    		int cnt = _Q_PENDING_LOOPS;
    		val = atomic_cond_read_relaxed(&lock->val,
    					       (VAL != _Q_PENDING_VAL) || !cnt--);
    	}

<img src="https://lihanlu.bj.bcebos.com/qs_6.png@!rdb" class="full-image" />

如果 pending 或者 tail 不为0，说明不是第一个等锁 cpu，跳转到标号 queue 排队。

kernel/locking/qspinlock.c:

	/*
	 * If we observe any contention; queue.
	 */
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

<img src="https://lihanlu.bj.bcebos.com/qs_47.png@!rdb" class="full-image" />


走到这里，如果没有其他 cpu 竞争，内存中的 `lock->val` 的值应该为 `[0, 0, *]`，无论 locked 位是否为 0 都先将 pending 位无条件置1。`queued_fetch_set_pending_acquire` 函数用于设置 pending 位，并返回内存中 lock 旧值。如果存在竞争 `lock->val` 可能为其他值。

kernel/locking/qspinlock.c:

	/*
	 * trylock || pending
	 *
	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
	 */
	val = queued_fetch_set_pending_acquire(lock);

<img src="https://lihanlu.bj.bcebos.com/qs_10.png@!rdb" class="full-image" />

如果刚刚调用 `queued_fetch_set_pending_acquire` 返回 lock 的旧值中 tail 或者 pending 位不为0，说明多个 cpu 争用，如果 pending 为0则要撤销刚设置的 pending 位，避免影响后续锁的状态迁移，然后跳转标号 queue。

kernel/locking/qspinlock.c:

	/*
	 * If we observe contention, there is a concurrent locker.
	 *
	 * Undo and queue; our setting of PENDING might have made the
	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
	 * on @next to become !NULL.
	 */
	if (unlikely(val & ~_Q_LOCKED_MASK)) {

		/* Undo PENDING if we set it. */
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);

		goto queue;
	}

<img src="https://lihanlu.bj.bcebos.com/qs_48.png@!rdb" class="full-image" />

如果没有争用，直接在全局锁的 locked 上自旋直到持锁者释放后，清空 pending 并设置 locked 位抢到锁。

kernel/locking/qspinlock.c:

	/*
	 * We're pending, wait for the owner to go away.
	 *
	 * 0,1,1 -> 0,1,0
	 *
	 * this wait loop must be a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because not all
	 * clear_pending_set_locked() implementations imply full
	 * barriers.
	 */
	if (val & _Q_LOCKED_MASK)
		atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_MASK));

	/*
	 * take ownership and clear the pending bit.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	clear_pending_set_locked(lock);
	qstat_inc(qstat_lock_pending, true);
	return;

<img src="https://lihanlu.bj.bcebos.com/qs_13.png@!rdb" class="full-image" />

下图是标号 queue 之前 `pending bit optimistic spinning` 的总结:

<img src="https://lihanlu.bj.bcebos.com/qs_22.png@!rdb" class="full-image" />

#### MCS queuing

**1. 标号 queue**

下面看下标号 queue 后的代码，首先从本 CPU 的 `mcs_nodes` 数组获取一个 node 并初始化，并构造 tail 信息。

kernel/locking/qspinlock.c:

    queue:
	qstat_inc(qstat_lock_slowpath, true);
    pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++;
	tail = encode_tail(smp_processor_id(), idx);

	node = grab_mcs_node(node, idx);

	/*
	 * Keep counts of non-zero index values:
	 */
	qstat_inc(qstat_lock_idx1 + idx - 1, idx);

	/*
	 * Ensure that we increment the head node->count before initialising
	 * the actual node. If the compiler is kind enough to reorder these
	 * stores, then an IRQ could overwrite our assignments.
	 */
	barrier();

	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);


如果在执行上面那些代码的过程中，锁已经处于空闲状态，那么本 CPU 可以获得锁，但是由于需要释放刚才已经创建的 node，所以还要跳转 release。

kernel/locking/qspinlock.c:

	/*
	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
	 * attempt the trylock once more in the hope someone let go while we
	 * weren't watching.
	 */
	if (queued_spin_trylock(lock))
		goto release;

<img src="https://lihanlu.bj.bcebos.com/qs_25.png@!rdb" class="full-image" />


否则把调用 `xchg_tail` 把全局 lock 的 tail 字段设置为本 CPU node 的信息，并且返回之前的 tail 节点信息。

kernel/locking/qspinlock.c:

	/*
	 * Ensure that the initialisation of @node is complete before we
	 * publish the updated tail via xchg_tail() and potentially link
	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
	 */
	smp_wmb();

	/*
	 * Publish the updated tail.
	 * We have already touched the queueing cacheline; don't bother with
	 * pending stuff.
	 *
	 * p,*,* -> n,*,*
	 */
	old = xchg_tail(lock, tail);
	next = NULL;

<img src="https://lihanlu.bj.bcebos.com/qs_26.png@!rdb" class="full-image" />

`old & QTAIL_MASK` 如果为真，说明 lock 之前的 tail 节点信息不为空，即本 CPU 的 node 并不是队首节点。获取前驱节点 prev，设置 `prev->next` 指向本 CPU node，然后调用 `arch_mcs_spin_lock_contended` 在本CPU的 `&node->locked` 字段自旋等待 `&node->locked` 变为 1 （`&node->locked` 表示 node 成为 队首)。

kernel/locking/qspinlock.c:

	/*
	 * if there was a previous node; link it and wait until reaching the
	 * head of the waitqueue.
	 */
	if (old & _Q_TAIL_MASK) { //如果不是队首节点
		prev = decode_tail(old); //获取前驱节点

		/* Link @node into the waitqueue. */
		WRITE_ONCE(prev->next, node); //将前驱节点的next指向本节点

		pv_wait_node(node, prev);
		arch_mcs_spin_lock_contended(&node->locked); //在本地node的locked自旋等待locked变为1

		/*
		 * While waiting for the MCS lock, the next pointer may have
		 * been set by another lock waiter. We optimistically load
		 * the next pointer & prefetch the cacheline for writing
		 * to reduce latency in the upcoming MCS unlock operation.
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next);
	}


变成队首节点后，等待现有持锁者释放锁，如果前面有 pending 的 cpu，还要等待它持锁-解锁完成，因此队首节点在 `locked_pending` 上自旋等待全局 lock 的 pending 和 locked 两个字段都被清空。

kernel/locking/qspinlock.c:

	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;
	//在locked_pending上自旋
	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));

<img src="https://lihanlu.bj.bcebos.com/qs_37.png@!rdb" class="full-image" />

**2. 标号 locked**

全局 lock 的 pending 和 locked 两个字段都被清空后，队首节点接可以持锁了，首先判断本 CPU node 是不是等待队列的队尾节点，如果是队尾节点就直接持锁后跳转到 release 释放 mcs node 结构。否则设置 lock 的 locked 字段，然后等待 next 节点更新前驱节点的 next 信息后，调用 `arch_mcs_spin_unlock_contended` 把下一个队列节点的 locked 字段置 1，使 next 节点成为新的等待队列的队首节点。

kernel/locking/qspinlock.c:

    locked:
	/*
	 * claim the lock:
	 *
	 * n,0,0 -> 0,0,1 : lock, uncontended
	 * *,*,0 -> *,*,1 : lock, contended
	 *
	 * If the queue head is the only one in the queue (lock value == tail)
	 * and nobody is pending, clear the tail code and grab the lock.
	 * Otherwise, we only need to grab the lock.
	 */

	/*
	 * In the PV case we might already have _Q_LOCKED_VAL set, because
	 * of lock stealing; therefore we must also allow:
	 *
	 * n,0,1 -> 0,0,1
	 *
	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
	 *       above wait condition, therefore any concurrent setting of
	 *       PENDING will make the uncontended transition fail.
	 */

    // 如果是等待队列的队尾节点，直接持锁后跳转到release释放mcs node结构
	if ((val & _Q_TAIL_MASK) == tail) {
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release; /* No contention */
	}

	/*
	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
	 * which will then detect the remaining tail and queue behind us
	 * ensuring we'll see a @next.
	 */

    // 否则先设置lock的locked字段持锁
	set_locked(lock);

    // 等待next节点更新前驱节点的next信息
	/*
	 * contended path; wait for next if not observed yet, release.
	 */
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

<img src="https://lihanlu.bj.bcebos.com/qs_44.png@!rdb" class="full-image" />

**3. 标号 release**

最后 release mcs 节点。

kernel/locking/qspinlock.c:

    release:
	/*
	 * release the node
	 */
	__this_cpu_dec(qnodes[0].mcs.count);


## 总结

下面的视频梳理了 `queued_spin_lock` 的流程。

<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe
src="https://lihanlu.bj.bcebos.com/qspinlock_540p.mp4" scrolling="no" border="0"
frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%;
height: 100%; left: 0; top: 0;"> </iframe></div>
